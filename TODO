Things to do (a.k.a. "road map"):
---------------------------------

a) get rid of separate RDF repositories and use only one, utilize
   contexts/named graphs instead or some other scheme - see for example
   Data Segmenting in Anzo
   (http://www.slideshare.net/LeeFeigenbaum/data-segmenting-in-anzo) and
   The Sesame 2 console: Creating Repositories, Loading Data with Context,
   SPARQL-Querying with Context
   (http://confuseddevelopment.blogspot.com/2007/10/sesame-2-console-creating-repositories.html)

b) some simple GUI for full-text searching using SOLR

   1) for organizations:
      searching using name and surname, organization name or ICO
      facets: sro, no, ...

   2) ...
   
   n) this also means creating proper SOLR schema!

c) GUI for "advanced search" using SOLR syntax with some "prepared examples"
   (in a form of link: after link is clicked, search term is filled in and
   search results are updated - each field and data type should be covered, like
   "zakladne imanie: from 10 to 100 â‚¬" etc.)

d) enhancing the Datanest CSV harvesters to be able to "re-run"
   and process only differences - i.e. we need to make them able to
   keep ODN copy of data up-to-date with Datanest "original"

e) adding more harvesters: direct harvesting of ORSR to get more data
   about companies, direct harvesting of procurement portals to have
   a shot by getting more data from the scanned documents themselves, ...

f) transform the existing application into something like container
   or whatever (maybe even using OSGI) so that it loads and runs
   Harvester and other components (as sort of plug-ins)

   i) at first, we need to only split the code into appropriate
      classes and packages, all in one project

   ii) later we can think about making it configurable so that ODN
       can be deployed in varying configurations (like somebody does
       not need certain harvesters and APIs so there would be a way to
       configure and deploy ODN to meet that criteria)

g) ...
